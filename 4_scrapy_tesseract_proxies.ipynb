{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy\n",
    "Scrapy nos permite manejar la request de manera paralela asincrónica. \n",
    "En vez de como le hacíamos con requests y BS de: \n",
    "1.\tenviar request\n",
    "2.\tprocesar la respuesta \n",
    "3.\thacer la próxima request \n",
    "con scrapy enviaremos y procesaremos de forma paralela. \n",
    "Otras ventajas: \n",
    "-\tya no debemos parsear con BS ya que podemos usar Xpath directamente \n",
    "-\tlimitar la cantidad de requests en paralelo \n",
    "-\tsetear demoras que nos indica el robots.tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practica analisis en paralelo \n",
    "crearemos un scrapper capaz de obtener el html de 2 paginas de todas las secciones del diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una herencia del objeto scrapy spider\n",
    "class Spider12(scrapy.Spider):\n",
    "    name = 'spider12'\n",
    "    # dominios a scrapear\n",
    "    allowed_domains = ['pagina12.com.ar']\n",
    "    # formato de archivo de salida\n",
    "    custom_settings = { 'FEED_FORMAT':'json',   \n",
    "                        'FEED_URI': 'resultados.json',\n",
    "                        'FEED_EXPORT_ENCODING': 'utf-8',\n",
    "                        'DEPTH_LIMIT' : 2}\n",
    "\n",
    "    # URLS a scrapear. son todas las secciones del diario \n",
    "    starts_urls = [ 'https://www.pagina12.com.ar/secciones/el-pais',\n",
    "                    'https://www.pagina12.com.ar/secciones/economia',\n",
    "                    'https://www.pagina12.com.ar/secciones/sociedad',\n",
    "                    'https://www.pagina12.com.ar/suplementos/cultura-y-espectaculos',\n",
    "                    'https://www.pagina12.com.ar/secciones/el-mundo',\n",
    "                    'https://www.pagina12.com.ar/secciones/deportes',\n",
    "                    'https://www.pagina12.com.ar/secciones/contratapa',\n",
    "                    'https://www.pagina12.com.ar/secciones/audiovisuales']\n",
    "    \n",
    "    # creamos un metodo obtener los links a cada nota \n",
    "    def parse(self, response): # response es el equivalente de hacer un requests.get()\n",
    "        # Listado de  links notas\n",
    "        links_notas = response.xpath(\"//h4[@class='title-list']/a/@href\").getall() # faltan h3,h2\n",
    "        for nota in links_notas: \n",
    "            # canalizar la respuesta hacia el metodo parse_nota\n",
    "            # yield == return, solo que direcciona el resultado a otro metodo\n",
    "            # for url in articles_title:\n",
    "            nota = 'https://www.pagina12.com.ar' + nota\n",
    "            yield response.follow(nota, callback=self.parse_nota)\n",
    "        # creamos accedemos al boton de siguiente pagina\n",
    "        next_page = 'https://www.pagina12.com.ar{}'.format(response.xpath('//a[@class=\"next\"]/@href').get())\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "    # creamos un metodo para analizar contenido de cada nota \n",
    "    def parse_nota(self, response):\n",
    "        title = response.xpath('//h1/text()').get()\n",
    "        summary = response.xpath('//h2/text()').get() \n",
    "        yield{\"url\": response.url,\n",
    "                \"titulo\": title,\n",
    "                \"resumen\": summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process = CrawlerProcess()\\nprocess.crawl(Spider12)\\nprocess.start()'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"process = CrawlerProcess()\n",
    "process.crawl(Spider12)\n",
    "process.start()\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proxys\n",
    "Como ya sabes un scrapper es un programa que automatice consultas(requests) a paginas web. Todas las consultas salen con desde un sola PC y por lo tanto tienen la misma IP, el servidor web detectara que hay un bot y te bloqueara cuando detecta:\n",
    "-\tAlta frecuencia de peticiones desde una sola IP\n",
    "Es por eso que necesitamos enmascarar las peticiones usando un proxy. El proxy es un intermediario entre nosostros y el servidor final, asi el servidor detectara la IP del proxy y no la nuestra. Podemos configurar al programa para que use varias proxys y engañe al servidor.\n",
    "\n",
    "Para saber tu IP publica con la que te identificas en la web  puedes usar : https://www.cualesmiip.com/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encontrar tu IP publica\n",
    "tambien podemos esta pagina que nos regresa directamente la IP, la usaremos para ver que pasa si ne nuestra request asignamos otra IP de un proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:46:55 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): ident.me:443\n",
      "2023-01-31 15:46:57 [urllib3.connectionpool] DEBUG: https://ident.me:443 \"GET / HTTP/1.1\" 200 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.68.173.193\n"
     ]
    }
   ],
   "source": [
    "print(requests.get('https://ident.me/').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_ip(url='https://ident.me/', proxies=None):   \n",
    "    # Hacemos la request al sitio\n",
    "    try:\n",
    "        resp = requests.get(url, proxies=proxies).text\n",
    "    except Exception as e:\n",
    "        print('Error haciendo la request.', e)\n",
    "        return None\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 16:24:22 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): ident.me:443\n",
      "2023-01-31 16:24:22 [urllib3.connectionpool] DEBUG: https://ident.me:443 \"GET / HTTP/1.1\" 200 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.68.173.193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'200.68.173.193'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_my_ip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configurar un proxy\n",
    "para configurar un proxy simplemente tenemos que crear un URL con:\n",
    "- protocolo de transferencia: http, https\n",
    "- IP publico del proxy: ejemplo 135.181.14.45\n",
    "- puerto del proxy: ejemplo 5959\n",
    "al final construyes un diccionario especificando un proxy http y otro https\n",
    "```python\n",
    "proxy_dict = {'http':'http://135.181.14.45:5959',\n",
    "              'https':'http://135.181.14.45:5959'}\n",
    "# ahora lo podemos pasar a la peticion\n",
    "r = requests.get(url, proxies=proxies)\n",
    "```\n",
    "arriba ocupamos la misma IP ya que esta aplicaba para el protocolo https y por lo tanto ya incluye http\n",
    "estos datos los podemos obtener de un pagina como: \n",
    "https://free-proxy-list.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 16:33:47 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): ident.me:443\n",
      "2023-01-31 16:33:49 [urllib3.connectionpool] DEBUG: https://ident.me:443 \"GET / HTTP/1.1\" 200 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.199.247.184'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_dict = {'http':'http://135.181.14.45:5959',\n",
    "            'https':'http://135.181.14.45:5959'}\n",
    "get_my_ip(proxies=proxy_dict)\n",
    "# ahora la IP con la que nos identificamos en internet es la del proxy y no la nuestra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proxys socks \n",
    "es lo mismo que arriba solo cambiamos la version de sock por el protocolo htpp pero ahora tienes que entrar a la pagina a consultar algun proxy sock https://www.socks-proxy.net/\n",
    "```python\n",
    "proxy_dict_socks = {'http':'socks4://138.97.116.171:50659',\n",
    "                    'https':'socks4://138.97.116.171:50659'}\n",
    "```\n",
    "recuerda que socks trabajan con protocolo TCP. Mas vijos pero lo podemos utilizar para cualquier tipo de tráfico ya sean páginas web, programas, torrents, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 16:42:00 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): ident.me:443\n",
      "2023-01-31 16:42:05 [urllib3.connectionpool] DEBUG: https://ident.me:443 \"GET / HTTP/1.1\" 200 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'36.67.27.189'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_dict_socks = {'http':'socks4://36.67.27.189:49524',\n",
    "                    'https':'socks4://36.67.27.189:49524'}\n",
    "get_my_ip(proxies=proxy_dict_socks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytesseract\n",
    "En esta clase veremos algunos ejemplos de utilización de Tesseract-OCR, una biblioteca creada por HP (y actualmente mantenida por Google) para realizar reconocimiento óptico de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tesserocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Panda\\Desktop\\platzi_code\\37\\37_web_scraping-_extraccion_de_datos_en_la_web\\4_scrapy_tesseract_proxies.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Panda/Desktop/platzi_code/37/37_web_scraping-_extraccion_de_datos_en_la_web/4_scrapy_tesseract_proxies.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtesserocr\u001b[39;00m \u001b[39m# Para hacer OCR\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Panda/Desktop/platzi_code/37/37_web_scraping-_extraccion_de_datos_en_la_web/4_scrapy_tesseract_proxies.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m \u001b[39m# Para hacer manipulación básica de imágenes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Panda/Desktop/platzi_code/37/37_web_scraping-_extraccion_de_datos_en_la_web/4_scrapy_tesseract_proxies.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \u001b[39m# Para visualizar imágenes\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tesserocr'"
     ]
    }
   ],
   "source": [
    "import tesserocr # Para hacer OCR\n",
    "import numpy as np # Para hacer manipulación básica de imágenes\n",
    "import matplotlib.pyplot as plt # Para visualizar imágenes\n",
    "from PIL import Image # Para cambiar el formato de archivos\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_largo = plt.imread('texto_largo.png')\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(texto_largo)\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ocr = tesserocr.file_to_text('texto_largo.png', lang='spa')\n",
    "print(texto_ocr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora intentemos con una imagen mas dificial la cual tiene 4 canales, las letras son color blanco y los bordes no estan bien definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('imagen de prueba.png')\n",
    "plt.imshow(img)\n",
    "texto_ocr = tesserocr.file_to_text('imagen de prueba.png', lang='spa')\n",
    "print(texto_ocr)\n",
    "# vemos como hay muchas fallas asi que debemos preprocesar las imagenes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = img[:,:,:3] # eliminamos el canal de transparencia\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_rgb[0,0,0]) # invertimos el color. para esto necesitamos saber el rango para hacer la correccion\n",
    "img_inv = 1 - img_rgb\n",
    "plt.imshow(img_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasamos de imagen rgb a b/n\n",
    "img_gr = img_inv.mean(axis=2) # sacamos el promedio de rbg \n",
    "plt.imshow(img_gr, cmap='Greys_r' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesseract no está preparado para trabajar con arrays de numpy. Debemos convertir el formato a Image\n",
    "img_pil = Image.fromarray(np.uint8(img_gr*255))\n",
    "print(tesserocr.image_to_text(img_pil, lang='spa'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho mejor, pero seguimos teniendo un problema ya que este curso es de Web Scraping, no Heb Scraping. Tal vez perdimos demasiada información al pasar la imagen a escala de grises. Intentemos con la imagen invertida en RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil_inv = Image.fromarray(np.uint8(img_inv*255))\n",
    "print(tesserocr.image_to_text(img_pil_inv, lang='spa'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89700b826ea957244dd80a5eea6853c4d4fde16e09f6b962977bd16c6aae5fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
